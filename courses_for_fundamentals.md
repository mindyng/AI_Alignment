* [What Eval org, Apollo Reasearch, is looking for](https://jobs.lever.co/apolloresearch/f66ef22f-f96a-42cb-ada9-fd48ce0d5fda)

* [x] from LLM Industry expert and OpenAI co-founder, Andrej Karpathy, [Deep Dive into LLMs like ChatGPT](https://www.youtube.com/watch?v=7xTGNNLPyMI) (how these are actually built, will help with pre, in- and post- processing of LLM's, what goes into prompt box, what is inside model, generation of response text)
 * [Tl;DR notes](https://anfalmushtaq.com/articles/deep-dive-into-llms-like-chatgpt-tldr)
* [x] [AI Safety Fundamentals](https://course.aisafetyfundamentals.com/alignment?session=1) / [AI Alignment Fast-Track Course](https://course.aisafetyfundamentals.com/alignment-fast-track?session=1)
* [x] [Stanford - Intro to AI Alignment](https://docs.google.com/document/d/1NX0DlZRzD3NP7tBeLjMh76w7-w2s8SxV3wj0P7EYpKY/edit?tab=t.0#heading=h.4p5dmkpp2yu9)
  * [ ] [Leveling Up in AI Safety Research Eng](https://docs.google.com/document/d/1b83_-eo9NEaKDKc9R3P5h5xkLImqMw8ADLmi__rkLo4/edit?tab=t.0#heading=h.fke682cxqkxr)  
* [x] [How to Get Into Independent Research on Alignment/Agency](https://www.lesswrong.com/posts/P3Yt66Wh5g7SbkKuT/how-to-get-into-independent-research-on-alignment-agency) (maybe read. dated and verbose. better off with Neel Nanda tweets)
* [ ] [Tips for Empirical Alignment Research](https://www.lesswrong.com/posts/dZFpEdKyb9Bf4xYn7/tips-for-empirical-alignment-research)
* [x] Upskilling

<img width="903" alt="Screenshot 2025-02-13 at 10 51 16â€¯AM" src="https://github.com/user-attachments/assets/b94f3f5a-802d-4c61-9f45-f50a71282abb" />
