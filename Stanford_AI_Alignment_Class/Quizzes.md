[Original Questions](https://docs.google.com/document/d/1qYG7Dyy9NqSq9x2F-kccgUy_Rsxiv5hKbqVe5z6LURE/edit?tab=t.0#heading=h.fke682cxqkxr)

1.2 Forecasting AI Advancements

Which of the following is NOT a reason that forecasting AI capabilities can be useful?
* [ ] To help predict when specific AI capabilities will arise
* [ ] To stimulate further research into AI safety
* [x] To predict the exact architecture of future AI systems
* [ ] To allow policymakers to make informed decisions about AI regulation
* [ ] To mitigate potential risks from advanced AI systems

What is one limitation of using the Turing Test as a measure of artificial general intelligence (AGI)?
* [x] It focuses only on evaluating a narrow range of linguistic tasks
* [ ] Modern non-AGI systems likely already pass versions of it
* [ ] Humans cannot pass it
* [ ] It was designed for older AI systems that are unlike today’s

What is the clearest advantage of using "transformative AI" as a definition compared to AGI?
* [ ] The transformer architecture is the dominant deep learning architecture.
* [x] The concept of a societal transition is easier to understand than general intelligence
* [ ] It focuses on clear real-world impacts rather than intrinsic system properties
* [ ] The technological revolutions provide historical precedents, but not so much for AGI
* [ ] It avoids anthropomorphizing AI systems

[Quiz Reading] According to Alexander 2022 summarizing Cotra 2020, why might FLOP alone not fundamentally drive the development of human-level AI?
* [ ] The quality of FLOP can vary significantly between different models
* [ ] The relationship between compute power (in FLOP) and AI development is not yet fully understood
* [ ] Understanding how to create machines that fundamentally think is as vital as compute power
* [ ] FLOP are becoming less common in measuring AI development

What does the "foom" debate refer to?
* [ ] Whether or not TAI is possible
* [ ] If scale is all you need to reach AGI
* [ ] Whether AI will be able to quickly self-improve
* [ ] The chance of catastrophe from TAI-like systems

How do "scaling laws" help enable predicting future AI capabilities?
* [x] To strongly relate training loss and computational resources
* [ ] They provide theoretical upper bounds on AI model sizes
* [ ] They model how hardware efficiency improves over time
* [ ] They help identify new algorithms and architectures

What phenomenon poses a challenge to predicting capabilities from scaling laws?
* [ ] Hardware efficiency improvements
* [ ] Algorithmic innovations
* [ ] AI itself improving AI development
* [ ] Emergent abilities
* [x] All of these answers

What approach did the Broken Neural Scaling Laws paper use to try to address the challenge of emergent abilities?
* [ ] Fitting piecewise functions to capture scaling law discontinuities
* [ ] Using easier synthetic tasks to smooth out performance jumps
* [ ] Measuring model internal representations instead of just behaviors
* [ ] Focusing on model scaling laws rather than task performance
* [ ] Switching to less brittle versions of performance metrics

Approximately how many FLOP were likely used to train AI systems like GPT-4?
* [ ] 10^8
* [ ] 10^12
* [ ] 10^20
* [ ] 10^26
* [ ] 10^30

According to the EpochAI survey, how do judgment-based AI forecasts tend to compare to model-based AI forecasts?
* [ ] Judgments and models are about the same
* [ ] Judgments tend to have longer forecasts than models
* [ ] Models tend to have longer forecasts than judgments 
* [ ] Models tend to have less uncertainty in their forecasts than judgments 

[Free Response] Let’s forecast some specific capabilities! 
Reflect on the current state of AI and where it’s going, and write one concrete prediction of something you think AI systems will be capable of that they aren’t yet AND another prediction of what you think they won’t be capable of by the end of this quarter (the final lecture). You won’t be graded on the accuracy of your predictions, though you should aim to be accurate.

Concrete predictions that we can compare at the end of the class.

2/11/25 prediction:
(+) better emotional intelligence?
(-) AGI

