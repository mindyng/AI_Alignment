### Organizations
* [aisafety.com - Comprehensive Collection of Orgs](https://www.aisafety.com/map)
* [Anthropic Research Team: Societal Impact, Alignment, Interpretability](https://www.anthropic.com/research)
* [Stanford Institute for Human-Centered Artificial Intelligence (HAI)](https://hai.stanford.edu/)

* Practically Seeing the Reason Why by Jail Breaking with [Gray Swan Arena](https://app.grayswan.ai/arena)
* [Special Competitive Studies Project](https://www.scsp.ai/)
* [80,000 Hours](https://80000hours.org)
  * https://80000hours.org/podcast/episodes/ajeya-cotra-accidentally-teaching-ai-to-deceive-us/
* [Center for Humane Technology](https://www.humanetech.com/)

### Mentoring
* [Supervised Program for Alignment Research](https://sparai.org/)
* [AI Safety Camp](https://www.aisafety.camp/home)
* [ML Alignment and Theory Scholars - MATS](https://www.matsprogram.org/faqs)

### Evaluation Org's to Reach Out to After Brushing Up on Fundamentals
* [https://metr.org/](https://hiring.metr.org/)
* [Apollo Research](https://www.apolloresearch.ai/)
  
### AI Governance
* [AI Gov](https://aigov.world/)
* U.S. AI Safety Institute aka AISI/National Institue of Standards and Technology aka NIST - **[Paul Christiano](https://www.linkedin.com/in/paul-christiano-5089211bb/)**
