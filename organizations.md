* [Anthropic Research Team: Societal Impact, Alignment, Interpretability](https://www.anthropic.com/research)
* [Center for AI Safety](https://www.safe.ai/careers)
* [Future of Life Institute](https://futureoflife.org/)
* [Alignment Research Center](https://www.alignment.org/)
* Practically Seeing the Reason Why by Jail Breaking with [Gray Swan Arena](https://app.grayswan.ai/arena)
* [Machine Intelligence Research Institute](https://intelligence.org/get-involved/)
* [Special Competitive Studies Project](https://www.scsp.ai/)
* [80,000 Hours](https://80000hours.org)
  * https://80000hours.org/podcast/episodes/ajeya-cotra-accidentally-teaching-ai-to-deceive-us/
  * https://80000hours.org/problem-profiles/artificial-intelligence/
  * https://80000hours.org/articles/ai-policy-guide/
* [Open Philanthropy](https://www.openphilanthropy.org/)
* [Center for Humane Technology](https://www.humanetech.com/)
* [Foresight Institute](https://foresight.org/technologies/secure-ai/)
* [Center for Human-Compatible AI](https://humancompatible.ai/about/)
* [FAR AI](https://www.youtube.com/@FARAIResearch/videos)
* [Stanford Institute for Human-Centered Artificial Intelligence (HAI)](https://hai.stanford.edu/)

* [aisafety.com - All I needed](https://www.aisafety.com/map)
* [https://aigov.world/](AI Gov)

After AI Safety Quest Nav Call:

Resources to Share
* [https://metr.org/](https://hiring.metr.org/)
* https://www.aisafety.com/map 
* https://aigov.world/
* https://arxiv.org/abs/2408.06292  (Research AI model unexpectedly attempts to modify its own code to extend runtime - Ars Technica)
Examples of specification gaming
* https://sparai.org/mentors 
Zvi writes a whole bunch about nonprofits in AI Safety and elsewhere
Lots of projects:
* Apollo Research
* https://www.matsprogram.org/ 
