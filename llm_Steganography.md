from this doc: https://docs.google.com/document/d/1ENXV0dU88lA40YIG_TqtTmfIPjwtDUBUQHpRd7RiMDU/edit?tab=t.0

[Mindy’s add-on to right above]

Concept: 
(1) So the black-box part of LLM’s is being solved by mech interp. My idea is focused on the untrustworthy parts of LLM’s output from their reasoning to get to an answer/their convo between LLM and LLM. This might be considered ‘noise’. This may be where this project can go such as get a "noise audit" - a methodology for measuring and quantifying judgment inconsistency 

“White-box detection strategies may become essential for mitigation” - Hidden in Plain Text: Emergence and Mitigation of Steganographic Collusion in LLM’s

Need help on research design, dataset type. Can help with evaluation, visualizations. 

Final product: paper

Connection to AI Safety/Contribution to Community: (1) detecting source (inside model-mech interp) of questionable steps in reasoning/decision-making or (2) systematically measuring noise (steganography eval’s) 

References/Resources: 
Preventing Large Language Models from Hiding Their Reasoning (encoded reasoning definition and paraphrasing as mitigation) ** includes datasets for steganography and paraphrasing! ** 

Metrics for evaluating CoT faithfulness: Measuring Faithfulness in CoT Reasoning

Next Steps: offering practical strategies for "decision hygiene" using insights from mech interp possibly
