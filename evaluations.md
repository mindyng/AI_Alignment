* famous paper: ["A Survey on Evaluation of Large Language Models"](https://arxiv.org/abs/2307.03109) [repo](https://github.com/MLGroupJLU/LLM-eval-survey)
* [Anthropic Evals](https://www.anthropic.com/research/statistical-approach-to-model-evals)
* OpenAI Evals
  * [repo](https://github.com/openai/evals)
  * [platform](https://platform.openai.com/docs/guides/evals)
* [HuggingFace Eval Guide](https://github.com/huggingface/evaluation-guidebook)
* [Eugene Yan Evals blog post](https://eugeneyan.com/writing/evals/)
* [AlpacaEval](https://github.com/tatsu-lab/alpaca_eval?tab=readme-ov-file#quick-start) - automatic evaluator for instruction-following language models. Human-validated, high-quality, cheap, and fast
* cool [LLM Eval tool](https://eugeneyan.com/writing/llm-evaluators/) based off of [Anthropic paper: Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073)
* what needs to be evaluated - [OpenAI's model spec](https://model-spec.openai.com/2025-02-12.html), which includes Stay in Bounds

* [Reddit post with various Eval tools](https://www.reddit.com/r/mlops/comments/1defvza/best_beginner_resources_for_llm_evaluation/)
* non-paper explanation of LLM Eval's in private sector and not one of the main-players - [Snorkel: How to Evaluate LLM Performance for Domain-Specific Use Cases](https://www.youtube.com/watch?v=ZHjulqB-4A0)
* Anthropic Eval publication: [Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations](https://arxiv.org/abs/2411.00640)
* [A Judge-free LLM Open-ended Generation Benchmark Based on the Distributional Hypothesis](https://arxiv.org/abs/2502.09316)

* yay! [python coded LLM eval article](https://medium.com/@jeffreyip54/llm-evaluation-metrics-everything-you-need-for-llm-evaluation-6b129157e33c)
* [LangWatch?](https://docs.langwatch.ai/optimization-studio/evaluating)
* [Future of LLM Systems Eval ](https://www.reddit.com/r/LocalLLaMA/comments/18z3ygo/the_future_of_llm_systems_evaluation/)
* [ArcAGI-Formal Benchmark for AGI Progress](https://arcprize.org/)
* [AI Security Institute (AISI)'s Inspect - An open-source framework for large language model evaluations](https://inspect.ai-safety-institute.org.uk/)
* [NIST Risk Management Framework](https://csrc.nist.gov/projects/risk-management)
* [ ] [Code Implementation of LLM Guardrail](https://github.com/mindyng/GuardReasoner) -- does not seem to have much traction
* most reliable LLM Leaderboard according to [Andrej Karpathy](https://x.com/karpathy/status/1917546757929722115) (May 2025)
