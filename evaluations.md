* famous paper: ["A Survey on Evaluation of Large Language Models"](https://arxiv.org/abs/2307.03109) [repo](https://github.com/MLGroupJLU/LLM-eval-survey)
* OpenAI Evals
  * [repo](https://github.com/openai/evals)
  * [platform](https://platform.openai.com/docs/guides/evals)
* [HuggingFace Eval Guide](https://github.com/huggingface/evaluation-guidebook)
* [Eugene Yan Evals blog post](https://eugeneyan.com/writing/evals/)
* [AlpacaEval](https://github.com/tatsu-lab/alpaca_eval?tab=readme-ov-file#quick-start) - automatic evaluator for instruction-following language models. Human-validated, high-quality, cheap, and fast
* cool [LLM Eval tool](https://eugeneyan.com/writing/llm-evaluators/) based off of [Anthropic paper: Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073)
* what needs to be evaluated - [OpenAI's model spec](https://model-spec.openai.com/2025-02-12.html), which includes Stay in Bounds

* [Reddit post with various Eval tools](https://www.reddit.com/r/mlops/comments/1defvza/best_beginner_resources_for_llm_evaluation/)
* non-paper explanation of LLM Eval's in private sector and not one of the main-players - [Snorkel: How to Evaluate LLM Performance for Domain-Specific Use Cases](https://www.youtube.com/watch?v=ZHjulqB-4A0)
* Anthropic Eval publication: [Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations](https://arxiv.org/abs/2411.00640)
* [A Judge-free LLM Open-ended Generation Benchmark Based on the Distributional Hypothesis](https://arxiv.org/abs/2502.09316)

* yay! [python coded LLM eval article](https://medium.com/@jeffreyip54/llm-evaluation-metrics-everything-you-need-for-llm-evaluation-6b129157e33c)
* [LangWatch?](https://docs.langwatch.ai/optimization-studio/evaluating)
* [Future of LLM Systems Eval ](https://www.reddit.com/r/LocalLLaMA/comments/18z3ygo/the_future_of_llm_systems_evaluation/)
* [ArcAGI-Formal Benchmark for AGI Progress](https://arcprize.org/)
* [AI Security Institute (AISI)'s Inspect - An open-source framework for large language model evaluations](https://inspect.ai-safety-institute.org.uk/)
* [NIST Risk Management Framework](https://csrc.nist.gov/projects/risk-management)
* [ ] [Code Implementation of LLM Guardrail](https://github.com/mindyng/GuardReasoner) -- does not seem to have much traction
* Groq (private industry) JD for SWE Intern for Model Evaluation Systems:

* Software Engineer, Model Evaluation Systems Intern
Remote (USA | Ontario, Canada) - 2/28/25

At Groq. We believe in an AI economy powered by human agency. We envision a world where AI is accessible to all, a world that demands processing power that is better, faster, and more affordable than is available today. AI applications are currently constrained by the limitations of the Graphics Processing Unit (GPU), a technology originally developed for the gaming market and soon to become the weakest link in the AI economy.

Enter Groq's LPU™ AI Inference Technology. Specifically engineered for the demands of large language models (LLMs), the Language Processing Unit outpaces the GPU in speed, power, efficiency, and cost-effectiveness. The quickest way to understand the opportunity is to watch the following talk – groq.link/scspdemo.

Why join Groq? AI will change humanity forever, and we believe preservation of human agency and self determination is only possible if AI is made affordably and universally accessible. Groq’s LPUs will power AI from an early stage, and you will get to leave your fingerprint on civilization.

Software Engineer, Model Evaluation Systems Intern (Winter 2025)
Remote (USA | Ontario, Canada)

Groq's Winter Internship Program runs from January - April and is full-time during the duration of the internship.

Mission: Build and optimize model evaluation systems to ensure that AI models deployed on Groq’s platform achieve best-in-class quality, throughput, and efficiency. Develop benchmarking frameworks, optimize model performance, and drive improvements that push the boundaries of evaluation systems.

Responsibilities & opportunities in this role:

Software Development: Write simple, concise, high-performance code in Python and Golang to support evaluation pipelines and tooling.
Model Evaluation & Benchmarking: Develop and implement benchmarking tools to assess the performance of models hosted on Groq, ensuring Groq’s AI infrastructure remains at the cutting edge of efficiency and performance.
Automation & Tooling: Build automated evaluation systems and testing infrastructure to streamline model deployment and validation processes.
System Integration & Testing: Work closely with cross-functional teams to integrate models into Groq’s infrastructure, ensuring seamless operation and accurate evaluation across various system configurations.
Data-Driven Insights: Collect and analyze performance data, providing actionable insights and recommendations for improving model performance, scalability, and reliability.
Ideal candidates have/are:

Current student pursuing a degree in computer science, engineering, or a related field or recent graduate (within 6 months)
A demonstrated interest in building or working with evaluation systems or working on novel applied AI projects.
Proficiency in Python and Golang, with strong software engineering fundamentals and best practices.
Self-Driven: Ability to see projects through end-to-end independently, taking initiative to optimize systems proactively.
Deep problem-solving skills — candidates should be able to tackle non-trivial challenges with minimal guidance.
Ability to work across unconventional AI problems, iterate quickly, and ship impactful solutions.
Contributions to open-source projects or prior startup experience is a big plus.
Must be authorized to work in the United States or Canada.
Attributes of a Groqster:

Humility - Egos are checked at the door
Collaborative & Team Savvy - We make up the smartest person in the room, together
Growth & Giver Mindset - Learn it all versus know it all, we share knowledge generously
Curious & Innovative - Take a creative approach to projects, problems, and design
Passion, Grit, & Boldness - no limit thinking, fueling informed risk taking
If this sounds like you, we’d love to hear from you!

Compensation: The US pay range for our technical internships is $30-$50 / per hour. The US pay range for our non-technical internships is $30-$40 / per hour. Individual compensation will be commensurate with the candidate’s qualifications and experience, country of internship and aligned with Groq’s internal leveling guidelines and benchmarks. #LI-DNI

Location: Groq is a geo-agnostic company, meaning you work where you are. Exceptional candidates will thrive in asynchronous partnerships and remote collaboration methods. Some roles may require being located near our primary sites, as indicated in the job description.  

At Groq: Our goal is to hire and promote an exceptional workforce as diverse as the global populations we serve. Groq is an equal opportunity employer committed to diversity, inclusion, and belonging in all aspects of our organization. We value and celebrate diversity in thought, beliefs, talent, expression, and backgrounds. We know that our individual differences make us better.
